import os
from utils import *
from normalize_data import normalize_data
from split_train_val import split_train_val
import numpy as np
import numba

def generate_truth(poses_file,calib_file,scan_folder,dst_folder,seq_idx):

  # load scan paths
  scan_paths = load_files(scan_folder)

  # load calibrations
  T_cam_velo = load_calib(calib_file)
  T_cam_velo = np.asarray(T_cam_velo).reshape((4, 4))
  T_velo_cam = np.linalg.inv(T_cam_velo)

  # load poses
  poses = load_poses(poses_file)
  pose0_inv = np.linalg.inv(poses[0])

  # for KITTI dataset, we need to convert the provided poses 
  # from the camera coordinate system into the LiDAR coordinate system  
  poses_new = []
  for pose in poses:
    poses_new.append(T_velo_cam.dot(pose0_inv).dot(pose).dot(T_cam_velo))
  poses = np.array(poses_new)

  n = len(poses)
  # n=10

  scan_points = []
  for i in range(n):
    scan_points.append(np.fromfile(scan_paths[i], dtype=np.float32).reshape((-1,4)))
    scan_points[i][:,-1] = 1
    if i % 100 == 0:
      print(f'loaded scan {i}')

  # generate overlap and yaw ground truth array
  ground_truth_mapping = np.empty((n*(n+1)//2,4))
  idx = 0
  for i in range(n):
    print('frame_idx:', i)
    ground_truth_mapping[idx:(idx+i+1)] = com_overlap_yaw(scan_points[:i+1], poses, i, range(i+1))
    idx += i+1
  
  # normalize the distribution of ground truth data
  dist_norm_data = normalize_data(ground_truth_mapping)
  # dist_norm_data = ground_truth_mapping
  
  # split ground truth for training and validation
  train_data, validation_data = split_train_val(dist_norm_data)
  
  # add sequence label to the data and save them as npz files
  # specify the goal folder
  dst_folder = os.path.join(dst_folder, 'ground_truth')
  try:
    os.stat(dst_folder)
    print('generating ground truth data in: ', dst_folder)
  except:
    print('creating new ground truth folder: ', dst_folder)
    os.mkdir(dst_folder)
    
  # training data
  train_seq = np.empty((train_data.shape[0], 2), dtype=object)
  train_seq[:] = seq_idx
  np.savez_compressed(dst_folder + '/train_set', overlaps=train_data, seq=train_seq)
  
  # validation data
  validation_seq = np.empty((validation_data.shape[0], 2), dtype=object)
  validation_seq[:] = seq_idx
  np.savez_compressed(dst_folder + '/validation_set', overlaps=validation_data, seq=validation_seq)
  
  # raw ground truth data, fully mapping, could be used for testing
  ground_truth_seq = np.empty((ground_truth_mapping.shape[0], 2), dtype=object)
  ground_truth_seq[:] = seq_idx
  np.savez_compressed(dst_folder + '/ground_truth_overlap_yaw', overlaps=ground_truth_mapping, seq=ground_truth_seq)
  
  print('Finish saving the ground truth data for training and testing at: ', dst_folder)


def com_overlap_yaw(scan_points, poses, frame_idx, ref_idxs, leg_output_width=360):
  """compute the overlap and yaw ground truth from the ground truth poses,
     which is used for OverlapNet training and testing.
     Args:
       scan_paths: paths of all raw LiDAR scans
       poses: ground-truth poses either given by the dataset or generated by SLAM or odometry
       frame_idx: the current frame index
     Returns:
       ground_truth_mapping: the ground truth overlap and yaw used for training OverlapNet,
                             where each row contains [current_frame_idx, reference_frame_idx, overlap, yaw]
  """
  # init ground truth overlap and yaw
  print('Start to compute ground truth overlap and yaw ...')
  overlaps = []
  yaw_idxs = []
  yaw_resolution = leg_output_width
  
  # we calculate the ground truth for one given frame only
  # generate range projection for the given frame
  current_range = range_projection_fast(scan_points[frame_idx])
  valid_num = np.count_nonzero(current_range > 0)
  current_pose = poses[frame_idx]

  for j in ref_idxs:

    reference_pose = poses[j]

    # calculate yaw angle
    relative_transform = np.linalg.inv(current_pose) @ reference_pose
    relative_rotation = relative_transform[:3, :3]
    _, _, yaw = euler_angles_from_rotation_matrix(relative_rotation)

    # discretize yaw angle and shift the 0 degree to the center to make the network easy to lean
    yaw_element_idx = int(- (yaw / np.pi) * yaw_resolution//2 + yaw_resolution//2)
    yaw_idxs.append(yaw_element_idx)

    relative_distance = np.linalg.norm(relative_transform[:3, -1])

    if relative_distance <= 100: # max_range * 2
      # generate range projection for the reference frame
      reference_points = scan_points[j]
      reference_points_in_current = reference_points @ relative_transform.astype(np.float32).T
      reference_range = range_projection_fast(reference_points_in_current)
      
      # calculate overlap
      overlap = np.count_nonzero(
        np.abs(reference_range[reference_range > 0] - current_range[reference_range > 0]) < 1
      ) / valid_num
      overlaps.append(overlap)
    else:
      overlaps.append(0.0)
  
  # ground truth format: each row contains [current_frame_idx, reference_frame_idx, overlap, yaw]
  ground_truth_mapping = np.empty((len(ref_idxs), 4))
  ground_truth_mapping[:, 0] = frame_idx
  ground_truth_mapping[:, 1] = np.arange(len(ref_idxs))
  ground_truth_mapping[:, 2] = overlaps
  ground_truth_mapping[:, 3] = yaw_idxs
  
  print('Finish generating ground_truth_mapping!')
  
  return ground_truth_mapping

@numba.njit
def fill(proj_range, proj_x, proj_y, depth):

  for i in range(depth.shape[0]):

    y,x = proj_y[i], proj_x[i]

    if proj_range[y,x] < 0 or depth[i] < proj_range[y,x]:
      proj_range[y,x] = depth[i]

  return proj_range

@numba.njit
def range_projection_fast(current_vertex):
  """ Project a pointcloud into a spherical projection, range image.
      Args:
        current_vertex: raw point clouds
      Returns: 
        proj_range: projected range image with depth, each pixel contains the corresponding depth
        proj_vertex: each pixel contains the corresponding point (x, y, z, 1)
        proj_intensity: each pixel contains the corresponding intensity
        proj_idx: each pixel contains the corresponding index of the point in the raw point cloud
  """

  fov_up=3.0
  fov_down=-25.0
  proj_H=64
  proj_W=900
  max_range=50

  fov_up = np.float32(fov_up / 180.0 * np.pi)  # field of view up in radians
  fov_down = np.float32(fov_down / 180.0 * np.pi)  # field of view down in radians
  fov = np.float32(np.abs(fov_down) + np.abs(fov_up))  # get field of view total in radians
  
  current_vertex = current_vertex[:, :3]

  # get depth of all points
  depth = np.sqrt(np.sum(current_vertex * current_vertex, axis=1))

  mask = (depth > 0) & (depth < max_range)
  current_vertex = current_vertex[mask]  # get rid of [0, 0, 0] points
  depth = depth[mask]
  
  # get scan components
  scan_x = current_vertex[:, 0]
  scan_y = current_vertex[:, 1]
  scan_z = current_vertex[:, 2]
  
  # get angles of all points
  yaw = -np.arctan2(scan_y, scan_x)
  pitch = np.arcsin(scan_z / depth)
  
  # get projections in image coords
  proj_x = yaw * (np.float32(0.5) / np.float32(np.pi)) + np.float32(0.5)  # in [0.0, 1.0]
  proj_y = 1 - (pitch + np.abs(fov_down)) * (1 / fov)  # in [0.0, 1.0]
  
  # scale to image size using angular resolution
  proj_x *= proj_W  # in [0.0, W]
  proj_y *= proj_H  # in [0.0, H]
  
  # round and clamp for use as index
  proj_x = np.clip(proj_x.astype(np.int32), 0, proj_W - 1) # in [0,W-1]
  proj_y = np.clip(proj_y.astype(np.int32), 0, proj_H - 1) # in [0,H-1]
  
  proj_range = np.full((proj_H, proj_W), -1, dtype=np.float32)  # [H,W] range (-1 is no data)

  proj_range = fill(proj_range, proj_x, proj_y, depth)

  return proj_range


if __name__ == "__main__":

  # sequences = [f"{i:02d}" for i in range(11)]
  # sequences = ["07"]
  # sequences = ["03","04","05","06","08","09","10"]
  sequences = ["00", "01", "02"]
  base_path = 'data/'

  for s in sequences:  
    generate_truth(
      poses_file=os.path.join(base_path, 'poses', f'{s}.txt'),
      calib_file=os.path.join(base_path, 'calib', s, 'calib.txt'),
      scan_folder=os.path.join(base_path, 'sequences', s, 'velodyne'),
      dst_folder=os.path.join(base_path, 'sequences', s),
      seq_idx=s
    )